---
layout: page
title: projects
permalink: /projects/
description: 
nav: true
nav_order: 3
display_categories: [work, fun]
horizontal: false
---

<h2>
<div style="text-align: center;">
<span style="color:blue">R</span>obust and <span style="color:blue">E</span>fficient Deep Learning based <span style="color:blue">A</span>udio-<span style="color:blue">VI</span>sual <span style="color:blue">S</span>peech <span style="color:blue">E</span>nhancement
</div>
</h2>

<div style="text-align: center;">
<img src="/files/av_se.png" alt="Audio-visual speech enhancement" width="600" height="200">
</div>

<div style="text-align: justify">
<b>REAVISE</b> (2023-2026) is a project funded by the <i>Agence Nationale de la Recherche</i> (<a href="https://anr.fr/">ANR</a>) via the <a href="https://anr.fr/en/call-for-proposals-details/call/programme-jeunes-chercheuses-et-jeunes-chercheurs-jcjc/">JCJC program</a>. The project aims to develop a <i>unified audio-visual speech enhancement (AVSE) framework</i> that robustly integrates acoustic data (noisy speech signal) with accompanying visual information (speakerâ€™s lip movements) in order to recover an intelligible, high-quality estimate of the clean speech signal with <i>low computational power</i> and <i>independently of the acoustic and visual noise environments</i>.
</div>

- [A quick introduction to REAVISE](/files/reavise.pdf){:target="\_blank" rel="noopener"}

## Job opportunities

<!-- <p style="font-size:12pt">
- <span style="color: blue"><b>PhD position</b></span>: See the project description & how to apply <a href="https://jobs.inria.fr/public/classic/en/offres/2023-05881">here</a>.
</p> -->

<p style="font-size:11pt">
- <span style="color: blue"><b>Postdoc/research engineer</b></span>: If you hold [or are about to get] a PhD degree, in audio/speech processing, machine learning, computer vision, or related fields, or you hold a Master's degree, please email me with your CV, motivation letter, and transcripts attached.
</p>

## Related publications

[1] Z. Kang, M. Sadeghi, R. Horaud, and X. Alameda-Pineda, **Expression-preserving face frontalization improves visually assisted speech processing**, International Journal of Computer Vision (IJCV), 2022. [[Paper](https://hal.archives-ouvertes.fr/hal-03902610){:target="\_blank" rel="noopener"}]

[2] M. Sadeghi, S. Leglaive, X. Alameda-Pineda, L. Girin, and R. Horaud, **Audio-visual Speech Enhancement Using Conditional Variational Auto-Encoders**, IEEE Transactions on Audio, Speech and Language Processing, vol. 28, pp. 1788- 1800, May 2020. [[Code](https://github.com/msaadeghii/avse-vae){:target="\_blank" rel="noopener"}]

[3] M. Sadeghi and X. Alameda-Pineda, **Mixture of Inference Networks for VAE-based Audio-visual Speech Enhancement**, IEEE Transactions on Signal Processing, vol. 69, pp. 1899-1909, March 2021. [[Code](https://github.com/msaadeghii/min-vae){:target="\_blank" rel="noopener"}]

[4] M. Sadeghi and X. Alameda-Pineda, **Switching Variational Auto-Encoders for Noise-Agnostic Audio-visual Speech Enhancement**, in IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), Toronto, Ontario, Canada, June 2021. [[Slides](/files/SwVAE_slides.pdf){:target="\_blank" rel="noopener"}, [Poster](/files/SwVAE_poster.pdf){:target="\_blank" rel="noopener"}]
